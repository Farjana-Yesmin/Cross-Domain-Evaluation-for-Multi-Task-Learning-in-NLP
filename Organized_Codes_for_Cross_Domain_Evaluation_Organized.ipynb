{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ac0877",
   "metadata": {},
   "source": [
    "# Supplementary Code for Cross-Domain Evaluation\n",
    "\n",
    "This notebook includes all steps used for data processing, model training, and evaluation in the research paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f38ea",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n",
    "\n",
    "The following libraries are essential for data processing, model building, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62cca3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKlP1VzgqrMO",
    "outputId": "66a81462-b7e9-4e8b-b5d9-6328a6728727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement fftooy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for fftooy\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets fftooy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from math import pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e4ff0",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "This section covers loading the dataset and preprocessing steps necessary for preparing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b5cd3",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load datasets from different domains and preprocess them as needed for cross-domain evaluation. This process ensures the model can generalize across varied linguistic or contextual features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9ae4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILRGnNdGhLj2",
    "outputId": "91be7ba7-3b57-4d40-a004-b913c4946e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research-paper/:\n",
      "notebooks  README.md  requirements.txt\tresults  scripts\n",
      "\n",
      "research-paper/notebooks:\n",
      "\n",
      "research-paper/results:\n",
      "figures  models\n",
      "\n",
      "research-paper/results/figures:\n",
      "\n",
      "research-paper/results/models:\n",
      "\n",
      "research-paper/scripts:\n",
      "evaluate_model.py  preprocess_data.py  train_model.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create main project directory\n",
    "os.makedirs(\"research-paper/notebooks\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/scripts\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/results/figures\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/results/models\", exist_ok=True)\n",
    "\n",
    "# Create placeholder files\n",
    "open(\"research-paper/requirements.txt\", \"w\").close()\n",
    "open(\"research-paper/README.md\", \"w\").close()\n",
    "\n",
    "# Create some Python script files in the 'scripts' folder\n",
    "open(\"research-paper/scripts/train_model.py\", \"w\").close()\n",
    "open(\"research-paper/scripts/evaluate_model.py\", \"w\").close()\n",
    "open(\"research-paper/scripts/preprocess_data.py\", \"w\").close()\n",
    "\n",
    "# Confirm directory structure\n",
    "!ls -R research-paper/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e076e",
   "metadata": {
    "id": "t2VlD8bbiaOi"
   },
   "outputs": [],
   "source": [
    "# Creating a README.md file\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "    # Research Paper: Bias Detection and Fairness Analysis in Object Detection\n",
    "\n",
    "    ## Overview\n",
    "    This research investigates bias detection and fairness analysis in object detection and image classification using the Open Images V7 dataset. The study evaluates model fairness on selected object classes: person, car, dog, cat, and chair.\n",
    "\n",
    "    ## Project Structure\n",
    "\n",
    "    - `notebooks/`: Jupyter notebooks for model training, testing, and experiments.\n",
    "      - `model_training.ipynb`: Notebook with model training and evaluation.\n",
    "    - `scripts/`: Python scripts for data processing, training, and evaluation.\n",
    "      - `train_model.py`: Script for model training.\n",
    "      - `evaluate_model.py`: Script for evaluation.\n",
    "      - `preprocess_data.py`: Script for preprocessing dataset.\n",
    "    - `results/`: Directory for storing results.\n",
    "      - `figures/`: Directory for storing charts, graphs, and visualizations.\n",
    "      - `models/`: Directory for saving trained models.\n",
    "      - `metrics.txt`: File for storing model metrics and performance.\n",
    "\n",
    "    ## Requirements\n",
    "    - Python 3.x\n",
    "    - Huggingface Transformers\n",
    "    - PyTorch\n",
    "    - Datasets library\n",
    "    - Other dependencies in `requirements.txt`\n",
    "\n",
    "    ## Installation\n",
    "\n",
    "    To install the necessary dependencies:\n",
    "\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "    ## Training the Model\n",
    "\n",
    "    To start training, run the following:\n",
    "\n",
    "    ```bash\n",
    "    python scripts/train_model.py\n",
    "    ```\n",
    "\n",
    "    ## Evaluation\n",
    "\n",
    "    After training, you can evaluate the model by running:\n",
    "\n",
    "    ```bash\n",
    "    python scripts/evaluate_model.py\n",
    "    ```\n",
    "\n",
    "    ## License\n",
    "    Include any licensing information if necessary.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7143f8",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "The following cells implement model training and optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0413ba",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-Domain Evaluation\n",
    "\n",
    "We evaluate model performance across different domains using metrics like accuracy, precision, and recall. This section provides insights into how well the model can adapt and perform on domains outside its training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62cca3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKlP1VzgqrMO",
    "outputId": "66a81462-b7e9-4e8b-b5d9-6328a6728727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement fftooy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for fftooy\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets fftooy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from math import pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd44487",
   "metadata": {
    "id": "jjwVR3WEuufp"
   },
   "outputs": [],
   "source": [
    "# Install required packages if not already done\n",
    "!pip install transformers wandb -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf6261",
   "metadata": {
    "id": "2vUuu09vwDbk"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set pad_token_id to eos_token_id to avoid padding token issues\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3ff73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "c8MJ6s7nZkaV",
    "outputId": "d43de86d-86b3-46c0-9872-66f4be4d3abf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.796723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_tokenizer/tokenizer_config.json',\n",
       " './fine_tuned_tokenizer/special_tokens_map.json',\n",
       " './fine_tuned_tokenizer/vocab.txt',\n",
       " './fine_tuned_tokenizer/added_tokens.json',\n",
       " './fine_tuned_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define a custom collate function to ensure that labels are passed to the model as tensors\n",
    "def collate_fn(batch):\n",
    "    # Make sure the 'labels' column is included during training and convert to tensor\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
    "    token_type_ids = torch.tensor([item['token_type_ids'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "\n",
    "    # Return the dictionary as required by the model\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Define the Trainer with the collate function\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    data_collator=collate_fn,            # Use the custom collate function\n",
    "    train_dataset=tokenized_datasets['train'],    # Training dataset\n",
    "    eval_dataset=tokenized_datasets['validation'],  # Validation dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer\n",
    "    compute_metrics=None,                # Optional: You can add metrics computation\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb67d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "YkfYpjzfajDy",
    "outputId": "270bd39e-e662-4a68-bb4b-29a309df84eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.080412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.280854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.48189441363016766, metrics={'train_runtime': 16.862, 'train_samples_per_second': 0.178, 'train_steps_per_second': 0.178, 'total_flos': 7708331700.0, 'train_loss': 0.48189441363016766, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    eval_strategy=\"epoch\",           # Evaluation strategy (updated parameter)\n",
    "    logging_dir='./logs',            # Directory for logs\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    "    save_steps=500,                  # Save the model every 500 steps\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # Batch size per device during evaluation\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,  # You can add custom metrics computation if needed\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f2282",
   "metadata": {
    "id": "AyuVpRlhceRW"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56c5d3",
   "metadata": {
    "id": "ZuS9PPSfdXEL"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_tokenizer')\n",
    "\n",
    "# Import metrics\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "f1_metric = load(\"f1\")\n",
    "precision_metric = load(\"precision\")\n",
    "recall_metric = load(\"recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e9d71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQlmI4djdry9",
    "outputId": "0d56b8fc-2c03-472e-a73c-789f1061259c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from evaluate import load  # Import from the evaluate library\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_tokenizer')\n",
    "\n",
    "# Load the metric\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "# Define a function to perform evaluation\n",
    "def evaluate_model(model, tokenizer, dataset, metric):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    for example in dataset:\n",
    "        inputs = tokenizer(example['text'], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        # Update the metric with predictions and labels\n",
    "        metric.add_batch(predictions=predictions, references=[example['labels']])\n",
    "\n",
    "    # Compute final results\n",
    "    result = metric.compute()\n",
    "    return result\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "test_results = evaluate_model(model, tokenizer, tokenized_datasets['test'], accuracy_metric)\n",
    "\n",
    "print(\"Test Accuracy:\", test_results[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9ae4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILRGnNdGhLj2",
    "outputId": "91be7ba7-3b57-4d40-a004-b913c4946e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research-paper/:\n",
      "notebooks  README.md  requirements.txt\tresults  scripts\n",
      "\n",
      "research-paper/notebooks:\n",
      "\n",
      "research-paper/results:\n",
      "figures  models\n",
      "\n",
      "research-paper/results/figures:\n",
      "\n",
      "research-paper/results/models:\n",
      "\n",
      "research-paper/scripts:\n",
      "evaluate_model.py  preprocess_data.py  train_model.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create main project directory\n",
    "os.makedirs(\"research-paper/notebooks\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/scripts\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/results/figures\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/results/models\", exist_ok=True)\n",
    "\n",
    "# Create placeholder files\n",
    "open(\"research-paper/requirements.txt\", \"w\").close()\n",
    "open(\"research-paper/README.md\", \"w\").close()\n",
    "\n",
    "# Create some Python script files in the 'scripts' folder\n",
    "open(\"research-paper/scripts/train_model.py\", \"w\").close()\n",
    "open(\"research-paper/scripts/evaluate_model.py\", \"w\").close()\n",
    "open(\"research-paper/scripts/preprocess_data.py\", \"w\").close()\n",
    "\n",
    "# Confirm directory structure\n",
    "!ls -R research-paper/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e076e",
   "metadata": {
    "id": "t2VlD8bbiaOi"
   },
   "outputs": [],
   "source": [
    "# Creating a README.md file\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "    # Research Paper: Bias Detection and Fairness Analysis in Object Detection\n",
    "\n",
    "    ## Overview\n",
    "    This research investigates bias detection and fairness analysis in object detection and image classification using the Open Images V7 dataset. The study evaluates model fairness on selected object classes: person, car, dog, cat, and chair.\n",
    "\n",
    "    ## Project Structure\n",
    "\n",
    "    - `notebooks/`: Jupyter notebooks for model training, testing, and experiments.\n",
    "      - `model_training.ipynb`: Notebook with model training and evaluation.\n",
    "    - `scripts/`: Python scripts for data processing, training, and evaluation.\n",
    "      - `train_model.py`: Script for model training.\n",
    "      - `evaluate_model.py`: Script for evaluation.\n",
    "      - `preprocess_data.py`: Script for preprocessing dataset.\n",
    "    - `results/`: Directory for storing results.\n",
    "      - `figures/`: Directory for storing charts, graphs, and visualizations.\n",
    "      - `models/`: Directory for saving trained models.\n",
    "      - `metrics.txt`: File for storing model metrics and performance.\n",
    "\n",
    "    ## Requirements\n",
    "    - Python 3.x\n",
    "    - Huggingface Transformers\n",
    "    - PyTorch\n",
    "    - Datasets library\n",
    "    - Other dependencies in `requirements.txt`\n",
    "\n",
    "    ## Installation\n",
    "\n",
    "    To install the necessary dependencies:\n",
    "\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "    ## Training the Model\n",
    "\n",
    "    To start training, run the following:\n",
    "\n",
    "    ```bash\n",
    "    python scripts/train_model.py\n",
    "    ```\n",
    "\n",
    "    ## Evaluation\n",
    "\n",
    "    After training, you can evaluate the model by running:\n",
    "\n",
    "    ```bash\n",
    "    python scripts/evaluate_model.py\n",
    "    ```\n",
    "\n",
    "    ## License\n",
    "    Include any licensing information if necessary.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2948a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Vrlbtd1oJKf",
    "outputId": "22224d56-71f4-4163-8980-1b2d1e834193"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a4c2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632,
     "referenced_widgets": [
      "7ed470a1f5eb4abb932168885a33030d",
      "7a632b4d039b4ac9b71137f819ba6512",
      "538a5550aea04d66ad447379e635c3d0",
      "98ebf5fdf5694c16a37b8d613cecf362",
      "ed247a6e75464f6e8686e8ac473d1974",
      "4082355bd63c4213a18a64194c8db891",
      "0743bf81b5ce4e409e1541594de9fc17",
      "a7eb7ee5b14b455b95ec95cc1753a119",
      "44bcfc2fc8e44a39927351d849e7306d",
      "799c4e16c40c4317ab829aa892652ff6",
      "5fa6bc948e154eafbfd724a6869e69f5",
      "4d0285b72a4549b9a966f758285fd43b",
      "468e2fb70db04dd5893d52cf44cf57c7",
      "fb09168dc0ea4bd38bf7ef75f7f27cb4",
      "2674b41e5fc148b787a071c28d15848b",
      "528dda5f139a4a0db7d03d86fd49597d",
      "530bce83af2c458cbe8da5bd465b3f47",
      "2300ce47f855470e972e49cb456a442b",
      "5b95eb7c6fb04045a497faa976823676",
      "d49d5909d9f54a5ea3274b2e23caf1b6",
      "572b9793928f48b59f703ef76ed670d8",
      "575c58f42e224dfdb6c9d30f3f12ac80",
      "b72c3ab7123c4d38bedf9cc0fa3a4fb4",
      "cec710ff874143df944a3dd3084a3ac7",
      "a2cab23f015d49609b41b83d5714bf3f",
      "14e94952069c4641bc7e3707691e9321",
      "ca6627e62e634688905306ff76d56ca3",
      "96a1faf60fd0413583e999f4de4e2522",
      "14b3a9b0c16c47afa9d350312a02b9dd",
      "f43669e1524f4007b782347916df9537",
      "b5ea3ddac1194ee9bc1b64f9c0e51e91",
      "747a031f99a84187981b0a59cb21ed82",
      "92e7c2e6436346f492e7f30496c2e88e",
      "c531f05bf98546af8fde5cf8db6f743a",
      "9b292a60223147c1b80631432a4d51ef",
      "0513c080085a41038a7c0e1bcfe3aaa8",
      "83ced447328c40eaa5b56c4d1f4a85be",
      "e9ade48397954c2e887e9e3ec168e399",
      "7eac4991bf164ce386b17bfa499ed97a",
      "6844186a6da5401997fa2a7bba37d7e1",
      "8e45c1630d5d43dda4c552902713dfdb",
      "72cd408f0fb44df8816834a6e7546171",
      "efd86c6d5b7b4d8ca4b3292f27f38035",
      "7698a3d1440f43f9abe5471930215221",
      "04af4cab1f674e6193aec1aaf5776780",
      "a6bcd8fe5e6d4197b977e0a7943ef8a1",
      "e203aa00e47d47ba9659ba65dea91741",
      "608f58734e234614b4c4fb1efffacb66",
      "62b4441fd6d9405dbf908af7a14a55a2",
      "99df6b107470444bb26cb7915c16c3ba",
      "9c48bcdd5ef34b248a915f4d415ca447",
      "d00db6ec60f34d539510706434b85dbb",
      "dbf6814aa03c47f1acfa45c6d1da3b2a",
      "84d4ef31b53a464794b2cf749b9162bd",
      "3d086266bfa34b218ebe4a83ff0a5471",
      "3942ab3dd2d54502a051b4fff1170c94",
      "4c95d173c548485f886047a50aa46025",
      "f5ad1e33219e4989ab638d78faeca552",
      "517280f478ee42eb87eeadb8416a2535",
      "49bcc594bd3d4e54ba90a36a5c469736",
      "b5751d152b1e43d6812d514304325017",
      "10c6c9a4140c471ea8a13d909ba80c26",
      "07de65185c7440c6a8bb07473333e2ee",
      "21392fae9429446fbdd240d9036055bf",
      "f0809b454abb48e38de507bff8a55aa3",
      "7c99a10c354846f7b7af98742fd79c26",
      "a432ac3760f14b8d9ef4aa8c31aa418b",
      "6041d04a8b144441a73446bbb2174612",
      "931505b81d314b7d889e27f103dc419b",
      "62e51a0ea25340308ca908abd84f92d7",
      "9adbe532359c47bb8271a71a53ea3886",
      "f278286b6e4848d69ce38840de39f161",
      "b852f52815504086a62518f1757b440c",
      "8f87c55239604bbe8522408743ae1d26",
      "b901cc211b9940a383f3690c3cd44129",
      "d49f7b2763654d33b580a3c359ad082e",
      "f6ec56f6fe5448c79ec21ce2e638ae69",
      "87a02c92f2ea46cfbc41e31c8babd68f",
      "545bb83444764012b5b71034714fdccd",
      "8cd84ad3e94941bc8f1541eb38f71f18",
      "3f36331b0f024d38bab2333825884357",
      "cb2f3152f10a4810bb5d1c70015f01b5",
      "4c72ffb0d4824a2a9c1bb00c63f99017",
      "3a51327eeeea4a4f9097c44125d5ce73",
      "2140f05f79d64d9b8cd4e67b6cb5ee42",
      "b8c5a956da2c44988be3985dbc36d06d",
      "2f7bb9a16ee44b688285eb71ad5d712c",
      "ec772140dcca4da9ba34db23ef9d5a1f",
      "bcacc335634044e28a42e0c050c0429c",
      "f230f84f06ce4957946cd1b58525e309",
      "8c1ef5b7a461455b87b5359779cef0ad",
      "72f40beb6d124fa19953cd4d368874be",
      "dd4609c38dbe456095d762c8a1fb1509",
      "47d7e23d92fa42ad9d17759a2cd8c677",
      "69b6abc5d17448e1b2579b959e708ccc",
      "8424afa52be54e39a4442143cff4add4",
      "92c231ded8b34a6bb3daeda9d61ad924",
      "1f4a817958ae4a2eabf93067c4c78d2e",
      "fe7cefb225714396b77d346660666b92",
      "15556e0ff39040e6a42ec31e254b8333",
      "a919ec859b5b47dcac09d5d54e02c5dd",
      "78ab11c7925d4b3c939193b028176cdb",
      "82d4dee253534a79ab396f28731e0071",
      "471700676eae4fb697a5861060f16d8f",
      "0687f86bedca41e88579b7006eed63c7",
      "fb275cea0ebf4dc59d07c302dbd48a52",
      "46ecc6c7d0b04de59baa4bff3d588e78",
      "7dd1aa4c69c646eca720ec163148966e",
      "c4c5c6af17554854a8cc60f8f39d2068",
      "3434322c83ca44baad6b3e8ee8bc06cf",
      "fcede7ff9b35417fb3e6b50a1eab7a8b",
      "1017f87535844c4fbadf7393667fd1a1",
      "cd99eafbf6ff4e489ac5653c85d7e7cf",
      "5184dab5f7ac4ec2b921cbc3b1e241f6",
      "9ba7e6f0706f46f4a531b88b3e777722",
      "78e3328554aa4f27ae7d84124a10c972",
      "d28a76b1800c4edc8f3039900ee9c753",
      "d8b4903f3a12465f8d71c07fa46a6b7b",
      "4046cb3260ef4a18b3397bd2222ecc97",
      "7a235616f68a4e38a1f662f16e64c1b1",
      "44949cbd174b41e195088dd15c0ba641",
      "fdc61975cac045a3881bd54514a8443d",
      "f7c473263b434d32906c628d1968d0b8",
      "627d3c06961b4d60a232249e43ea0d7e",
      "ce3d2df3680e492183c865930e894452",
      "8e9f2f10480c4ebab3c1c8f9de20f808",
      "2644b50d85d24b92bf4dda2b6b43c364",
      "fdd27242b5c74225ae1592b09dd5c618",
      "a27f246ffa3544a2acd750a8c5c83803",
      "89fc1e35b9e94709a5f1f100e17bb13d",
      "6f2044e874a24aae8fc2bafa5754d693",
      "68e3ff6010004c93afac88bf48086f65",
      "d9932c81fd954516bf75d3805cb9510c",
      "7d32902042394bed94a792a6e15a1776",
      "e9b159c2595a45e2ab3dc61f29e76868",
      "2f46e5a3293d4e8fb20bd02fd10874ad",
      "8b2e00c092da4de7b9123ce47a3ddc2e",
      "cf196e9f8f174a94a9fe8c06b8a83676",
      "1feb64e7fc274fa9abfaf5908e697c6e",
      "85ad71fa411043bb985ea89117a82dd1",
      "8d8ee358e5f04a068d260e0b7a97446f",
      "18ffa9a2fe5a4e8b9e2837f7dfcd03a9",
      "77f3a2719f3941c4a68f896b67481202"
     ]
    },
    "id": "kq-lJ1nY8qoh",
    "outputId": "cb79e134-1943-40ba-83e8-c781eeab1169"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed470a1f5eb4abb932168885a33030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0285b72a4549b9a966f758285fd43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72c3ab7123c4d38bedf9cc0fa3a4fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c531f05bf98546af8fde5cf8db6f743a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04af4cab1f674e6193aec1aaf5776780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3942ab3dd2d54502a051b4fff1170c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a432ac3760f14b8d9ef4aa8c31aa418b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a02c92f2ea46cfbc41e31c8babd68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcacc335634044e28a42e0c050c0429c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15556e0ff39040e6a42ec31e254b8333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcede7ff9b35417fb3e6b50a1eab7a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc61975cac045a3881bd54514a8443d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9932c81fd954516bf75d3805cb9510c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the dataset\n",
    "test_dataset = load_dataset(\"glue\", \"mrpc\", split=\"test\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512fc20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "2elXwXKq8_jp",
    "outputId": "8d15830c-4180-4609-e257-f65aa8c03f74"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-03ced067c970>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=test_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec28ad",
   "metadata": {
    "id": "P338pF1M9N_c"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the dataset\n",
    "test_dataset = load_dataset(\"glue\", \"mrpc\", split=\"test\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load the accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define the evaluation function\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Set the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7ad8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632,
     "referenced_widgets": [
      "83efe433a8c54e4cb4de283937ace98f",
      "4ea50229a7fb4c3680f04b4e3f992ec6",
      "211715dcbffe44dd97fd7a2efe51e1d8",
      "c4032d5b07a348189ed093bebec94458",
      "354878fbdeb84c2bb80835d9b7868b97",
      "897371a4cebd43bda253e9ae8010319f",
      "8c376688827a435680f8a34ac7f90a26",
      "b0325c4bbc6b43db88910c2a590a9a21",
      "4b40e47bb8024b169a5a7659a3aa0793",
      "91ee542cc6e84050a1f0ed20fb553745",
      "ffb871aec5a746c0a555721901a08fda",
      "aea2c158934749cab9313d813d938427",
      "b20e1b0f6758413893f4c01d330c570b",
      "b01277b28b464f4ba5195febbbb6c762",
      "fabf30b811a842c0a95464572ea7594d",
      "f95b73e896af4f8b9068292b520c3e03",
      "c572ed7c4ccf4d94a7661feafd0761be",
      "2db6ec96516447feb686dd5971ddb54f",
      "fc2ed3682c0f4119b786eaae51bba55e",
      "b5579703f697406f802d7874bcea4891",
      "24e1b3a0cf784d9ebe07389b97299e0b",
      "a8628977cb204e4581159f2db33baf02",
      "67494ab82c994443b44da87dd5881479",
      "0c92edce0f1a4b3a959146e55a449216",
      "5cb7e600b70749b98ea3f03286ed41ac",
      "7ea617acb52948abb0db07021bc1a825",
      "580e708493324d63a9523f8a0104df0a",
      "9b5c91fd7fbd472e8fd5b8dfa36cbe94",
      "4f7780770ca146429fe3f008e79f0aee",
      "ffa6815b66654aefa38197545d79bcab",
      "07edbbfbc4064156aa3c7886750f6d8a",
      "3994d25778404058bd295578cab9f1ff",
      "53c5a19aaa8648b5ae56d12e743742fa",
      "6aaa5f1b15554de0b3e706b5213e99ce",
      "8533cc1a962f45f08124f65bb86847ca",
      "4acbfa07982142dfb91661144f8d4759",
      "561737276d69430d829f24f43a77280f",
      "80369862df854680b6fdcd529c8d7700",
      "abe0fdb7abf64446a15275767560917b",
      "3fe940e11cd74b5bb11dca84aa3358ac",
      "297d11d4143c4c70a4ee2da0a650d416",
      "838e6aea41fd4160995f9b0d4d9f1b45",
      "bfc8cebac6964b6db1564fdc4fb43056",
      "cfeee2b88f4f4cdc99a83b8b6e093c73",
      "dd03483f86364e69822bfb0a9786ffb5",
      "f9055e8d1a4a48fa8a105ddc675e8d49",
      "768c923120b849d1b7cd9a125d37cd6f",
      "2c3c94e86abe4446b1188160e95798a6",
      "0875c69b797d489f9009962f56cc2520",
      "9aacf98f67e74fcb84dd54ebf486405e",
      "4e33f850dbb54ee180814c5633154cbd",
      "d86adbb71097403ea31d1373376ef942",
      "a522e7a5cc3a48a5be43e867837fd69b",
      "e2f1455bee674f2594051e9db4d68eaf",
      "91c2d3be1cbd4dbe93cf829636294af8",
      "f76ecbc49ff149c695455f3e44d396bc",
      "098cc0458118413e916bda32225e660e",
      "f7048c4da47c414b96cb0424f6c5bef3",
      "4fbc89530cba475594bdc7a867f4d4e6",
      "5e928000781c4d2f9dcaba6373b7c9be",
      "93ccb27fb184408f80c645dfa32dfd88",
      "893756da3dc944cbb30d3cd6e5294da3",
      "8605bdf8665645748c67d741a4587bf2",
      "c02b9954db3c44e49d222a68e5ee7ab9",
      "d789dc3623a74b069e9fd7709589b156",
      "c35435a7d7c640c4973a3b8db96117c7",
      "7512447b7801415c89d1673735876e2f",
      "4d1ce3fbd40d401b9b27ba77113cb96c",
      "e87589b78eda4ce29f75c0c77b0729ee",
      "1c86c35071e7405f9d98d5238c657879",
      "0e5b50a3bc4e47cca20f094896df025d",
      "ee56d8c28f7b4a1aa46a427e73608844",
      "94c8371c6d914188a2b031bcbfbcbb0f",
      "c174bb9a629a4e63b3f7936e43af7558",
      "17237944b5eb4e3abdbeff7a5629862d",
      "11e09a88eaa54b4b90a45b1237947e4b",
      "3d4a6ada11284502abb7ec55e15ac68a",
      "6f7bde2d73e04e92bbceac384dfb09c5",
      "aec009bdff6a4c5eb96df2b014dd3483",
      "f6bc500c058843e2a688671c41e888be",
      "e1f60c0de9a444c3a732179fb8e45646",
      "ef3b5d7312c047e1b945fcbc321b9da5",
      "5c4682d77e314fdf8b22695eddbf3530",
      "e51f184502e54efe9224c44d732f4772",
      "8d362151520d446fad79de0590465e8e",
      "88a7cbab061e448aabdf15b9b813f387",
      "4314c64f77ca48deb93762e8a50ad47d",
      "72a29d326b2f474cba15ccf4ee8abbfd",
      "8762423d6db24c3193186dd42620c950",
      "68d729c412a944e4acd900c8653c9d5e",
      "a20650cc60f2480a9029fcbbae18c311",
      "f6cf93e925d840d6a144d98ff22f2487",
      "aecc62b2405549698a150dd6e79b1218",
      "9b48226556254c85b96d3584e2120b67",
      "1938cb3e08f2474980d75cffcab7ef4d",
      "8c050f7c0ac641c2aa83b94226483e20",
      "3e8dcf9c4e6d47129eacae1d29f3a386",
      "076d014e48884b93b81c0b77f7069ae3",
      "f3c85195e8f540d7aa22804166344cdf",
      "148d89a19be04715841bdfa51bd4e002",
      "77634ee426e14bf390d30f596dd0b0d3",
      "6994e4aeaa094663b222ed214940b235",
      "1c7d2374d1b145c6b8e83b17851638aa",
      "14bbc0663fb0421a8e0f270052093faf",
      "28cfa6bb1caa4f0b835db9b67f496de9",
      "26c91ee3d33a46df8dd6f46dd348de7b",
      "066ce8928ebb4a64bfdfeec464d6de5a",
      "df9817bdb44d477882a4d57f7b9cb18c",
      "699ff5c615aa472ba338a008a886c043",
      "a12a71bc4a9e4d31b86a056d2801135e",
      "8c7e784f098b45fd871f41a2fea99a16",
      "07a5a988d9504cf5ac3a2b0335976cb7",
      "a1273cbb404d48c390c38944ec007d18",
      "676ba4b2d3cf4d53b256135252c82bdb",
      "b30e1cceb7154054ae5514f007f57c1e",
      "aa606fde8d424780b2c25bb7c5b74c3d",
      "34134b4173b74fbda2ef6404d93b1879",
      "2e330225718646f0950f210b8da3a802",
      "f26f1de967b94d218c7febad59940ef4",
      "80dbe62ba1ce4212b74324d72fdbdce2",
      "f07b3a376673483b99236ca6283ab9bf",
      "12dfecc119d84e92a1fa6f0d97c233f1",
      "3b733340152b46a7a849b5a145da9b11",
      "bdb449e75d254f7a91637d78e6b99fee",
      "6c92c738be0b48e99b80c7661aa4c382",
      "8b489d00b2cd4fc0ba60bc819836cc53",
      "a05cb627ff1b460587819bff3383ec44",
      "aa43d0142a9949479b092ffd56338a70",
      "cb9a8a93a01144eca4c18d83080ca427",
      "b7a3f47355a94772a72d0fa9b00d7d7d",
      "5b013ba1e9974adf8c302cf14cb6c86c",
      "0913261ad3314336a76855aa0939058f",
      "9d3250c3e7074505a090e12fbea081c4",
      "6b0c6076747a4978a93456aabbe25572",
      "1c748ef869ae4982a5f9fefcb3e3c986",
      "26f83d84ba934dc49c08052533ea0b5d",
      "878aa127eef444abb3f29ffc1371ae80",
      "fb74b006261a4589908822e5d5963379",
      "b073dd5011284b6b8fe42becd8c99470",
      "2ddc10e6d5b541518344df9569b3ffa5",
      "69049ab539d44ec7b4bb8b102a545095",
      "43b2e62888094cbca9ad27d500d67bf9",
      "c05cbfe55a544d36a97344d4baa339a8"
     ]
    },
    "id": "76DXg-9v-5X4",
    "outputId": "91744910-bf3a-44be-af14-fd29f2463bd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83efe433a8c54e4cb4de283937ace98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea2c158934749cab9313d813d938427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67494ab82c994443b44da87dd5881479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaa5f1b15554de0b3e706b5213e99ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd03483f86364e69822bfb0a9786ffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76ecbc49ff149c695455f3e44d396bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7512447b7801415c89d1673735876e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7bde2d73e04e92bbceac384dfb09c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8762423d6db24c3193186dd42620c950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148d89a19be04715841bdfa51bd4e002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7e784f098b45fd871f41a2fea99a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dfecc119d84e92a1fa6f0d97c233f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3250c3e7074505a090e12fbea081c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load dataset again\n",
    "test_dataset = load_dataset(\"glue\", \"mrpc\", split=\"test\")\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Reapply tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fb1b3",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "This section evaluates model performance using relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63c911",
   "metadata": {},
   "source": [
    "\n",
    "# Cross-Domain Evaluation for Multi-Task Learning in NLP\n",
    "\n",
    "This notebook evaluates cross-domain generalization and robustness of models in multi-task NLP. We focus on evaluating model behavior and biases across multiple domains (e.g., Domain A, Domain B, Domain C).\n",
    "\n",
    "**Objectives**:\n",
    "1. Examine the generalization performance of models on unseen domains.\n",
    "2. Assess any emerging biases when transferring tasks across domains.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0413ba",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-Domain Evaluation\n",
    "\n",
    "We evaluate model performance across different domains using metrics like accuracy, precision, and recall. This section provides insights into how well the model can adapt and perform on domains outside its training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8105b3",
   "metadata": {},
   "source": [
    "\n",
    "## Results and Analysis\n",
    "\n",
    "Visualize the metrics to understand any biases or domain-specific performance variations. Highlight key insights from the cross-domain analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8127488",
   "metadata": {
    "id": "2vaYQBVWA2Ty"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Setup training arguments with a custom run name\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=4,   # batch size for training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    run_name=\"gpt2_experiment_v1\",   # Unique run name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3ff73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "c8MJ6s7nZkaV",
    "outputId": "d43de86d-86b3-46c0-9872-66f4be4d3abf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.796723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_tokenizer/tokenizer_config.json',\n",
       " './fine_tuned_tokenizer/special_tokens_map.json',\n",
       " './fine_tuned_tokenizer/vocab.txt',\n",
       " './fine_tuned_tokenizer/added_tokens.json',\n",
       " './fine_tuned_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define a custom collate function to ensure that labels are passed to the model as tensors\n",
    "def collate_fn(batch):\n",
    "    # Make sure the 'labels' column is included during training and convert to tensor\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
    "    token_type_ids = torch.tensor([item['token_type_ids'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "\n",
    "    # Return the dictionary as required by the model\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Define the Trainer with the collate function\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    data_collator=collate_fn,            # Use the custom collate function\n",
    "    train_dataset=tokenized_datasets['train'],    # Training dataset\n",
    "    eval_dataset=tokenized_datasets['validation'],  # Validation dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer\n",
    "    compute_metrics=None,                # Optional: You can add metrics computation\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb67d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "YkfYpjzfajDy",
    "outputId": "270bd39e-e662-4a68-bb4b-29a309df84eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.080412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.280854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.48189441363016766, metrics={'train_runtime': 16.862, 'train_samples_per_second': 0.178, 'train_steps_per_second': 0.178, 'total_flos': 7708331700.0, 'train_loss': 0.48189441363016766, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    eval_strategy=\"epoch\",           # Evaluation strategy (updated parameter)\n",
    "    logging_dir='./logs',            # Directory for logs\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    "    save_steps=500,                  # Save the model every 500 steps\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # Batch size per device during evaluation\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,  # You can add custom metrics computation if needed\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32b489",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FzoGfDqcZet",
    "outputId": "01904547-0ded-4dc4-d264-3daecb5eb749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f2282",
   "metadata": {
    "id": "AyuVpRlhceRW"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4590166",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "87522c1033cd4013bbaf393b4bc23e57",
      "b9454de93b974366aab1b55d690c7aed",
      "a902308a644c4e65aae636767e5b522c",
      "968e772d4a95463f9f92c5cef450d26c",
      "4b7fa13b41594ea0b4ba35116ad44ee3",
      "b625030bcad5401eac872589eac53049",
      "b7ae3aa7cd7e49bd8a2c03faed4a4104",
      "1384dd0c59084560812a3b952dd29025",
      "d679943967ea45a8a425c4ab0a6d3ecc",
      "cfd44cda76194163b3fae5bb7c69bf9b",
      "d225c7ed259341be80a94f9ea66bcf42",
      "2a188556e410499ea3bca6a94a1dcb44",
      "80646f5a8fc64c8e8d1c96596ee5a1ef",
      "4e99d7e3574f4b73bedfac088e52ddec",
      "a372b35645134734904bb3121b0363d7",
      "301d9252db2b4ccb84511b4afce23275",
      "9e8aa2fc1dcc48f49ec563326118d467",
      "306f8d73a2ba4e53bb0c3df8e91869a0",
      "8d92916847244645addb3437b47dd6eb",
      "2be42f74ef4044609e5d549561ad84d6",
      "ab3cfbbb743e438da00483bddc8febe8",
      "f83e5388c2aa4ad482cf2f9a6138e038",
      "62261df613dd4d9a9a8836889fabaeb6",
      "9abfee70a44b40cbac5b76b632cada60",
      "6553c10a36c545d6aeabf967326fb84c",
      "4484ca7db71b43f98b2b1919b6a3e86a",
      "7cb007e97a0b4da4a66f5ccdb5a7d9d4",
      "b7e82f8374ff4a7883a2364fb374a8a3",
      "a9dab7639fce4d7a824ba71ae33d09e8",
      "21854c0f4e52479a92e8f243385eac5a",
      "faf13f26296c4fd1be3eb895b22c3c52",
      "aa37d7fe48bb4bd1970b52961ad483d7",
      "6308a485be0a4a27afec77326fd67166",
      "e43cfc2dd4f2460d9bfda2d26763b2af",
      "8ecffbe19fd84296a99970497a83c312",
      "c1d895539dd14efbb9d41a47860575e4",
      "02ef03c2e1fb4ce89154c08f2374d4d8",
      "fe4238efcab543e3ad0292d16ef9a761",
      "a511909e1c3c4403afbbe9abb43e34e6",
      "b1a12323802c46f79065282f189673b1",
      "ba777a26bc6742a0816823b80eaa6b8b",
      "916b131811bd43e3a9b22e04d1f5c89b",
      "343c23d7bc204d20bae97c6d9bff4791",
      "e061db259425483586f3b74175fbf454"
     ]
    },
    "id": "pU_EPPn-ci_q",
    "outputId": "3f11e533-d560-4e6e-e398-53fbad5678bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87522c1033cd4013bbaf393b4bc23e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a188556e410499ea3bca6a94a1dcb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62261df613dd4d9a9a8836889fabaeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43cfc2dd4f2460d9bfda2d26763b2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_metric = load(\"accuracy\")\n",
    "f1_metric = load(\"f1\")\n",
    "precision_metric = load(\"precision\")\n",
    "recall_metric = load(\"recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905f62c",
   "metadata": {
    "id": "vJzbu-EKcn2h"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)['accuracy']\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average='weighted')['f1']\n",
    "    precision = precision_metric.compute(predictions=preds, references=labels, average='weighted')['precision']\n",
    "    recall = recall_metric.compute(predictions=preds, references=labels, average='weighted')['recall']\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7ca57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dbTEOXfdRq7",
    "outputId": "d6951ca7-20c7-4997-e1e5-1cf465a075b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666a9f9",
   "metadata": {
    "id": "vMdcaYJAdUkr"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56c5d3",
   "metadata": {
    "id": "ZuS9PPSfdXEL"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_tokenizer')\n",
    "\n",
    "# Import metrics\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "f1_metric = load(\"f1\")\n",
    "precision_metric = load(\"precision\")\n",
    "recall_metric = load(\"recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e9d71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQlmI4djdry9",
    "outputId": "0d56b8fc-2c03-472e-a73c-789f1061259c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from evaluate import load  # Import from the evaluate library\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_tokenizer')\n",
    "\n",
    "# Load the metric\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "# Define a function to perform evaluation\n",
    "def evaluate_model(model, tokenizer, dataset, metric):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    for example in dataset:\n",
    "        inputs = tokenizer(example['text'], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        # Update the metric with predictions and labels\n",
    "        metric.add_batch(predictions=predictions, references=[example['labels']])\n",
    "\n",
    "    # Compute final results\n",
    "    result = metric.compute()\n",
    "    return result\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "test_results = evaluate_model(model, tokenizer, tokenized_datasets['test'], accuracy_metric)\n",
    "\n",
    "print(\"Test Accuracy:\", test_results[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9ae4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILRGnNdGhLj2",
    "outputId": "91be7ba7-3b57-4d40-a004-b913c4946e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research-paper/:\n",
      "notebooks  README.md  requirements.txt\tresults  scripts\n",
      "\n",
      "research-paper/notebooks:\n",
      "\n",
      "research-paper/results:\n",
      "figures  models\n",
      "\n",
      "research-paper/results/figures:\n",
      "\n",
      "research-paper/results/models:\n",
      "\n",
      "research-paper/scripts:\n",
      "evaluate_model.py  preprocess_data.py  train_model.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create main project directory\n",
    "os.makedirs(\"research-paper/notebooks\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/scripts\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/results/figures\", exist_ok=True)\n",
    "os.makedirs(\"research-paper/results/models\", exist_ok=True)\n",
    "\n",
    "# Create placeholder files\n",
    "open(\"research-paper/requirements.txt\", \"w\").close()\n",
    "open(\"research-paper/README.md\", \"w\").close()\n",
    "\n",
    "# Create some Python script files in the 'scripts' folder\n",
    "open(\"research-paper/scripts/train_model.py\", \"w\").close()\n",
    "open(\"research-paper/scripts/evaluate_model.py\", \"w\").close()\n",
    "open(\"research-paper/scripts/preprocess_data.py\", \"w\").close()\n",
    "\n",
    "# Confirm directory structure\n",
    "!ls -R research-paper/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e076e",
   "metadata": {
    "id": "t2VlD8bbiaOi"
   },
   "outputs": [],
   "source": [
    "# Creating a README.md file\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "    # Research Paper: Bias Detection and Fairness Analysis in Object Detection\n",
    "\n",
    "    ## Overview\n",
    "    This research investigates bias detection and fairness analysis in object detection and image classification using the Open Images V7 dataset. The study evaluates model fairness on selected object classes: person, car, dog, cat, and chair.\n",
    "\n",
    "    ## Project Structure\n",
    "\n",
    "    - `notebooks/`: Jupyter notebooks for model training, testing, and experiments.\n",
    "      - `model_training.ipynb`: Notebook with model training and evaluation.\n",
    "    - `scripts/`: Python scripts for data processing, training, and evaluation.\n",
    "      - `train_model.py`: Script for model training.\n",
    "      - `evaluate_model.py`: Script for evaluation.\n",
    "      - `preprocess_data.py`: Script for preprocessing dataset.\n",
    "    - `results/`: Directory for storing results.\n",
    "      - `figures/`: Directory for storing charts, graphs, and visualizations.\n",
    "      - `models/`: Directory for saving trained models.\n",
    "      - `metrics.txt`: File for storing model metrics and performance.\n",
    "\n",
    "    ## Requirements\n",
    "    - Python 3.x\n",
    "    - Huggingface Transformers\n",
    "    - PyTorch\n",
    "    - Datasets library\n",
    "    - Other dependencies in `requirements.txt`\n",
    "\n",
    "    ## Installation\n",
    "\n",
    "    To install the necessary dependencies:\n",
    "\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "    ## Training the Model\n",
    "\n",
    "    To start training, run the following:\n",
    "\n",
    "    ```bash\n",
    "    python scripts/train_model.py\n",
    "    ```\n",
    "\n",
    "    ## Evaluation\n",
    "\n",
    "    After training, you can evaluate the model by running:\n",
    "\n",
    "    ```bash\n",
    "    python scripts/evaluate_model.py\n",
    "    ```\n",
    "\n",
    "    ## License\n",
    "    Include any licensing information if necessary.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3b7e8",
   "metadata": {
    "id": "I8Tap5H_qNTS"
   },
   "outputs": [],
   "source": [
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d529213",
   "metadata": {
    "id": "GYcb7cnUqQY-"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280f8d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1iHrhE04Qkj",
    "outputId": "7022a1be-fd84-4890-cefb-0f102241b5d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff2130",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKRssVTO5JHr",
    "outputId": "bf740425-3724-4658-aedc-83890335374e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The virtual environment was not created successfully because ensurepip is not\n",
      "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
      "package using the following command.\n",
      "\n",
      "    apt install python3.10-venv\n",
      "\n",
      "You may need to use sudo with that command.  After installing the python3-venv\n",
      "package, recreate your virtual environment.\n",
      "\n",
      "Failing command: /content/myenv/bin/python3\n",
      "\n",
      "/bin/bash: line 1: myenv/bin/activate: No such file or directory\n",
      "Requirement already satisfied: fsspec==2024.9.0 in /usr/local/lib/python3.10/dist-packages (2024.9.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Create a virtual environment\n",
    "!python3 -m venv myenv\n",
    "\n",
    "# Activate the virtual environment\n",
    "!source myenv/bin/activate\n",
    "\n",
    "# Then install your required packages\n",
    "!pip install fsspec==2024.9.0\n",
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb461e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fqzsfsfh7nyL",
    "outputId": "4b2aacd0-ec57-4140-b8db-7ff5dfc7a829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fsspec==2024.9.0\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.9.0\n",
      "Collecting gcsfs==2024.9.0\n",
      "  Downloading gcsfs-2024.9.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (3.10.10)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (4.4.2)\n",
      "Collecting fsspec==2024.6.1 (from gcsfs==2024.9.0)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (1.2.1)\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (2.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.9.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.9.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.9.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs==2024.9.0) (1.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs==2024.9.0) (2.19.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs==2024.9.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs==2024.9.0) (2.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (2024.8.30)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.9.0) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.9.0) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.9.0) (1.25.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs==2024.9.0) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (4.12.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2024.9.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2024.9.0) (3.2.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (0.2.0)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'gcsfs' candidate (version 2024.9.0 at https://files.pythonhosted.org/packages/46/87/165a7fc37498576b1622fe69b1915d60cf145bb5d7c4a2ee9943bad44040/gcsfs-2024.9.0-py2.py3-none-any.whl (from https://pypi.org/simple/gcsfs/) (requires-python:>=3.8))\n",
      "Reason for being yanked: requirements incorrect\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading gcsfs-2024.9.0-py2.py3-none-any.whl (34 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: fsspec, gcsfs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "  Attempting uninstall: gcsfs\n",
      "    Found existing installation: gcsfs 2024.10.0\n",
      "    Uninstalling gcsfs-2024.10.0:\n",
      "      Successfully uninstalled gcsfs-2024.10.0\n",
      "Successfully installed fsspec-2024.6.1 gcsfs-2024.9.0\n",
      "Requirement already satisfied: datasets==3.1.0 in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.1.0) (0.2.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fsspec==2024.9.0\n",
    "!pip install gcsfs==2024.9.0\n",
    "!pip install datasets==3.1.0\n",
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580cbd7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuliB-ra8KVn",
    "outputId": "1b688b88-6df7-49b5-f231-3149745114e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fsspec==2024.9.0\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: fsspec\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 1.25.0 requires gcsfs>=2023.3.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.9.0\n",
      "Collecting gcsfs==2024.9.0\n",
      "  Using cached gcsfs-2024.9.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (3.10.10)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (4.4.2)\n",
      "Collecting fsspec==2024.6.1 (from gcsfs==2024.9.0)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (1.2.1)\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (2.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.9.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.9.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.9.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs==2024.9.0) (1.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs==2024.9.0) (2.19.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs==2024.9.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs==2024.9.0) (2.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==2024.9.0) (2024.8.30)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.9.0) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.9.0) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.9.0) (1.25.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs==2024.9.0) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (4.12.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2024.9.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2024.9.0) (3.2.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.9.0) (0.2.0)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'gcsfs' candidate (version 2024.9.0 at https://files.pythonhosted.org/packages/46/87/165a7fc37498576b1622fe69b1915d60cf145bb5d7c4a2ee9943bad44040/gcsfs-2024.9.0-py2.py3-none-any.whl (from https://pypi.org/simple/gcsfs/) (requires-python:>=3.8))\n",
      "Reason for being yanked: requirements incorrect\u001b[0m\u001b[33m\n",
      "\u001b[0mUsing cached gcsfs-2024.9.0-py2.py3-none-any.whl (34 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: fsspec, gcsfs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "Successfully installed fsspec-2024.6.1 gcsfs-2024.9.0\n",
      "Requirement already satisfied: datasets==3.1.0 in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.1.0) (0.2.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fsspec==2024.9.0  # Install compatible fsspec version for datasets\n",
    "!pip install gcsfs==2024.9.0   # Install compatible gcsfs version for bigframes\n",
    "!pip install datasets==3.1.0   # Install datasets package\n",
    "!pip install evaluate          # Reinstall evaluate if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a0462",
   "metadata": {
    "id": "xAQAdy4u8nt8"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79d3d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "9UH32LdI8316",
    "outputId": "71e699dc-4146-44b1-d493-c0c541f0b571"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c50690a7d96a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca08b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "Herko0qz88vl",
    "outputId": "71736724-65fd-4ad5-80c7-f75616204d98"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4c111ca3d4f6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mper_device_eval_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogging_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./logs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512fc20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "2elXwXKq8_jp",
    "outputId": "8d15830c-4180-4609-e257-f65aa8c03f74"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-03ced067c970>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=test_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d5ae0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "KPVtSSeU9FUm",
    "outputId": "b6b0c682-c936-42c5-8460-d2cf5de6dcae"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-54a2f04e84c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluation results: {eval_results}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec28ad",
   "metadata": {
    "id": "P338pF1M9N_c"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the dataset\n",
    "test_dataset = load_dataset(\"glue\", \"mrpc\", split=\"test\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load the accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define the evaluation function\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Set the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8f146",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tx3AQ7bc-vHj",
    "outputId": "b664bad1-de92-4a56-c7df-7c21b15cf7c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Requirement already satisfied: fsspec==2024.9.0 in /usr/local/lib/python3.10/dist-packages (2024.9.0)\n",
      "Collecting gcsfs==2024.9.0\n",
      "  Downloading gcsfs-2024.9.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (3.10.10)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.9.0) (4.4.2)\n",
      "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install fsspec==2024.9.0 and gcsfs==2024.9.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested fsspec==2024.9.0\n",
      "    gcsfs 2024.9.0 depends on fsspec==2024.6.1\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate torch\n",
    "!pip install fsspec==2024.9.0 gcsfs==2024.9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edd46ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "4fSgNetuzpVv",
    "outputId": "242219aa-8f1b-4a47-a326-7ef0228b1d47"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-f7bc0b71b0c4>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-f7bc0b71b0c4>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    This notebook evaluates cross-domain generalization and robustness of models in multi-task NLP. We focus on domains A, B, and C, analyzing model behav\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Cross-Domain Evaluation for Multi-Task Learning in NLP\n",
    "\n",
    "This notebook evaluates cross-domain generalization and robustness of models in multi-task NLP. We focus on domains A, B, and C, analyzing model behav\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d6a60",
   "metadata": {},
   "source": [
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "Below we install and import necessary libraries and authenticate access if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18571e",
   "metadata": {},
   "source": [
    "\n",
    "## Model Loading\n",
    "\n",
    "Load the fine-tuned models for each domain. Ensure paths to locally saved models or configurations are set correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a86ee",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Summarize the findings, including the model's robustness and generalization across domains, limitations observed, and potential areas for improvement in handling domain shift in NLP tasks.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
